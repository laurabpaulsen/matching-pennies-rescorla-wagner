---
title: "Untitled"
output: html_document
date: "2024-02-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(rstan)
library(rethinking)
library(ggplot2)

```

**Rescorla-Wagner** in R.

Simulate some data first.

```{r}
# Simulate opponent's choices
opponent_choices <- rbinom(n = 100, size = 1, prob = 0.7)

# Display the first few opponent choices
head(opponent_choices)

```


```{r}

# Parameters
tau <- 2  # Temperature parameter for softmax function
alpha <- 0.6  # Learning rate

# Initialize values
choice <- numeric(length(opponent_choices))
choice[1] <- rbinom(1, size = 1, prob = 0.5)  # First choice randomly sampled

# Initialize values for the Rescorla-Wagner model
v <- c(0.5, 0.5)  # Initial values for the agent's choices (heads and tails)
rewards <- numeric(length(opponent_choices))

# Simulate agent's responses
for (t in 2:length(opponent_choices)) {  # Starting from the second round
  # Calculate reward
  if (opponent_choices[t - 1] == choice[t - 1]) {
    reward <- 1
  } else {
    reward <- 0
  }
  rewards[t - 1] <- reward
  
  # Rescorla-Wagner model
  pe <- rewards[t - 1] - v[choice[t - 1] + 1]
  
  v[choice[t - 1] + 1] <- v[choice[t - 1] + 1] + alpha * pe
  
  # Update choice based on softmax
  prob_head <- 1 - exp(tau * v[1]) / (exp(tau * v[1]) + exp(tau * v[2]))
  choice[t] <- rbinom(1, size = 1, prob = prob_head)
}

# Display the rewards, choices, and updated values
print(rewards)
print(choice)
print(v)

choice = choice + 1

```

Let's do a prior predictive check.

```{r}

prior_alpha = rbeta(1000, shape1 = 1, shape2 = 1)
dens(prior_alpha)

prior_tau = runif(1000, 0, 1)
dens(prior_tau)

```


```{r}

## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h.
data <- list(
  nTrials = 100,  # n of trials
  choice = choice,
  reward = rewards 
)

```

```{r}

modelFile <- 'my_RW.stan'

nIter     <- 2000
nChains   <- 4 
nWarmup   <- floor(nIter/2)
nThin     <- 1

fit_rl <- stan(modelFile, 
               data    = data, 
               chains  = nChains,
               iter    = nIter,
               warmup  = nWarmup,
               thin    = nThin,
               init    = "random",
               seed    = 1450154626)

```

```{r}

print(fit_rl)
    
plot_trace_excl_warm_up <- stan_trace(fit_rl, pars = c('alpha','tau'), inc_warmup = F)
plot_dens <- stan_plot(fit_rl, pars=c('alpha','tau'), show_density=T, fill_color = 'skyblue')

```

Let's do some posterior predictive checks.

```{r}



```


Now let's generate data for hierarchical modelling, so for N participants. And our parameters *alpha* and *tau* will be sampled from respective distributions.

```{r}

# Display the first few opponent choices
head(opponent_choices)

# Parameters for simulation
num_participants <- 10
num_rounds <- 100
alpha_mu <- 0.6
alpha_sd <- 0.2
tau_mu <- 1.5
tau_sd <- 0.2

# Simulate opponent's choices
opponent_choices <- rbinom(n = num_rounds, size = 1, prob = 0.7)

# Initialize matrices to store choices and rewards for each participant
choices <- matrix(NA, nrow = num_participants, ncol = num_rounds)
rewards <- matrix(NA, nrow = num_participants, ncol = num_rounds)

# Generate alpha and tau values for each participant
alpha_values <- rnorm(num_participants, mean = alpha_mu, sd = alpha_sd)
tau_values <- rnorm(num_participants, mean = tau_mu, sd = tau_sd)

# Simulate agent's responses for each participant
for (participant in 1:num_participants) {
  # Initialize values for the Rescorla-Wagner model
  v <- c(0.5, 0.5)  # Initial values for the agent's choices (heads and tails)
  
  # Initialize choice for the first round randomly sampled
  choices[participant, 1] <- rbinom(1, size = 1, prob = 0.5)
  
  # Simulate agent's responses for subsequent rounds
  for (t in 2:num_rounds) {
    # Calculate reward
    if (opponent_choices[t - 1] == choices[participant, t - 1]) {
      reward <- 1
    } else {
      reward <- 0
    }
    
    rewards[participant, t - 1] <- reward
    
    # Rescorla-Wagner model
    pe <- reward - v[choices[participant, t - 1] + 1]
    v[choices[participant, t - 1] + 1] <- v[choices[participant, t - 1] + 1] + alpha_values[participant] * pe
    
    # Update choice based on softmax
    prob_head <- 1 - exp(tau_values[participant] * v[1]) / (exp(tau_values[participant] * v[1]) + exp(tau_values[participant] * v[2]))
    choices[participant, t] <- rbinom(1, size = 1, prob = prob_head)
  }
}

rewards[1, 100] = 1
rewards[2, 100] = 1
rewards[3, 100] = 1
rewards[4, 100] = 1
rewards[5, 100] = 1
rewards[6, 100] = 1
rewards[7, 100] = 1
rewards[8, 100] = 1
rewards[9, 100] = 1
rewards[10, 100] = 1


# Display the rewards, choices, and updated values for one participant (e.g., the first participant)
print(rewards[1, ])
print(choices[1, ])
print(v)

choices = choices + 1
```

Make a data list for Stan.

```{r}

## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h.
data <- list(
  nSubjects = num_participants,
  nTrials = num_rounds,
  choice = choices,
  reward = rewards 
)

```

Run the Stan model.

```{r}

modelFile <- 'my_RW_hierarchical.stan'

nIter     <- 2000
nChains   <- 4 
nWarmup   <- floor(nIter/2)
nThin     <- 1

fit_rl <- stan(modelFile, 
               data    = data, 
               chains  = nChains,
               iter    = nIter,
               warmup  = nWarmup,
               thin    = nThin,
               init    = "random",
               seed    = 1450154626)

```



```{r}

print(fit_rl)

```

