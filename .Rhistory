#alpha <- 0.3  # Learning rate
# # High-High
# tau <- 9
# alpha <- 0.8
# # Low-Low
# tau <- 0.5
# alpha <- 0.1
# # High-Low
# tau <- 9
# alpha <- 0.1
# Low-High
# tau <- 0.5
# alpha <- 0.8
# ------------------------------------
# Defining function for getting agent + opponent responses
simulate_agents_resp <- function(trials,              # Number of trials
alpha,               # Learning Rate
tau,                 # Inverse temperature
opponent_bias){      # Put opponent's bias here (e.g. 0.7, 0.5...)
# First, generate opponent's choices
opponent_choices <- rbinom(n = trials,
size = 1,
prob = opponent_bias)
# Initialize values
choice <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
choice[1] <- rbinom(1,
size = 1,
prob = 0.5)  # First choice randomly sampled
# ----- The Recorla-Wagner model
# Initialize values for the Rescorla-Wagner model
v <- c(0.5,
0.5)  # Initial values for the agent's choices (heads and tails)
rewards <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
# Simulate agent's responses
for (t in 2:length(opponent_choices)) {  # Starting from the second round
# Calculate reward
if (opponent_choices[t - 1] == choice[t - 1]) {
reward <- 1
} else {
reward <- 0
}
rewards[t - 1] <- reward
# Rescorla-Wagner model
pe <- rewards[t - 1] - v[choice[t - 1] + 1]
v[choice[t - 1] + 1] <- v[choice[t - 1] + 1] + alpha * pe
# Update choice based on softmax
prob_head <- 1 - exp(-tau * v[1]) / (exp(-tau * v[1]) + exp(-tau * v[2]))
choice[t] <- rbinom(1, size = 1, prob = prob_head)
}
# Display the rewards, choices, and updated values
print(rewards)
print(choice)
print(v)
choice = choice + 1
## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h.
data <- list(
nTrials = trials,  # n of trials
choice = choice,
reward = rewards)
return(data)
}
# Using the func to generate some data
data <- simulate_agents_resp(trials = 120,
alpha = 0.6,
tau = 2,
opponent_bias = 0.8)
# -------- PARAMS TO RUN SETTINGS, UNHASH TO GET SPECIFICS
# Parameters
#tau <- 2  # Temperature parameter for softmax function
#alpha <- 0.3  # Learning rate
# # High-High
# tau <- 9
# alpha <- 0.8
# # Low-Low
# tau <- 0.5
# alpha <- 0.1
# # High-Low
# tau <- 9
# alpha <- 0.1
# Low-High
# tau <- 0.5
# alpha <- 0.8
# ------------------------------------
# Defining function for getting agent + opponent responses
simulate_agents_resp <- function(trials,              # Number of trials
alpha,               # Learning Rate
tau,                 # Inverse temperature
opponent_bias){      # Put opponent's bias here (e.g. 0.7, 0.5...)
# First, generate opponent's choices
opponent_choices <- rbinom(n = trials,
size = 1,
prob = opponent_bias)
# Initialize values
choice <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
choice[1] <- rbinom(1,
size = 1,
prob = 0.5)  # First choice randomly sampled
# ----- The Recorla-Wagner model
# Initialize values for the Rescorla-Wagner model
v <- c(0.5,
0.5)  # Initial values for the agent's choices (heads and tails)
rewards <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
# Simulate agent's responses
for (t in 2:length(opponent_choices)) {  # Starting from the second round
# Calculate reward
if (opponent_choices[t - 1] == choice[t - 1]) {
reward <- 1
} else {
reward <- 0
}
rewards[t - 1] <- reward
# Rescorla-Wagner model
pe <- rewards[t - 1] - v[choice[t - 1] + 1]
v[choice[t - 1] + 1] <- v[choice[t - 1] + 1] + alpha * pe
# Update choice based on softmax
prob_head <- 1 - exp(-tau * v[1]) / (exp(-tau * v[1]) + exp(-tau * v[2]))
choice[t] <- rbinom(1, size = 1, prob = prob_head)
}
# Display the rewards, choices, and updated values
print(rewards)
print(choice)
print(v)
choice = choice + 1
## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h.
data <- list(
nTrials = trials,  # n of trials
choice = choice,
reward = rewards)
return(list(data = data, tau = tau, alpha = alpha))
}
# Using the func to generate some data
data <- simulate_agents_resp(trials = 120,
alpha = 0.6,
tau = 2,
opponent_bias = 0.8)
result = simulate_agents_resp(trials = 120,
alpha = 0.6,
tau = 2,
opponent_bias = 0.8)
data = result$data
alpha = result$alpha
tau = result$tau
modelFile <- 'RW.stan'
# Setting specs for modelling
nIter     <- 2000
nChains   <- 4
nWarmup   <- floor(nIter/2)
nThin     <- 1
# Timing statements
cat("Estimating", modelFile, "model... \n")
startTime = Sys.time(); print(startTime)
cat("Calling", nChains, "simulations in Stan... \n")
# Fitting model
fit_rl <- stan(modelFile,
data    = data,
chains  = nChains,
iter    = nIter,
warmup  = nWarmup,
thin    = nThin,
init    = "random",
seed    = 1450154626)
# Timing statements
cat("Finishing", modelFile, "model simulation ... \n")
endTime = Sys.time()
print(endTime)
# Corrected time difference display
time_difference <- endTime - startTime
cat("It took", as.numeric(time_difference, units = "secs"), "seconds\n")
# Get summary of model
print(fit_rl)
result = simulate_agents_resp(trials = 120,
alpha = 0.6,
tau = 2,
opponent_bias = 0.8)
data = result$data
alpha = result$alpha
tau = result$tau
modelFile <- 'RW.stan'
# Setting specs for modelling
nIter     <- 2000
nChains   <- 4
nWarmup   <- floor(nIter/2)
nThin     <- 1
# Timing statements
cat("Estimating", modelFile, "model... \n")
startTime = Sys.time(); print(startTime)
cat("Calling", nChains, "simulations in Stan... \n")
# Fitting model
fit_rl <- stan(modelFile,
data    = data,
chains  = nChains,
iter    = nIter,
warmup  = nWarmup,
thin    = nThin,
init    = "random",
seed    = 1450154626)
# Timing statements
cat("Finishing", modelFile, "model simulation ... \n")
endTime = Sys.time()
print(endTime)
# Corrected time difference display
time_difference <- endTime - startTime
cat("It took", as.numeric(time_difference, units = "secs"), "seconds\n")
# Get summary of model
print(fit_rl)
summary(fit_rl)
result = simulate_agents_resp(trials = 120,
alpha = 0.6,
tau = 2,
opponent_bias = 0.8)
data = result$data
alpha = result$alpha
tau = result$tau
modelFile <- 'RW.stan'
# Setting specs for modelling
nIter     <- 2000
nChains   <- 4
nWarmup   <- floor(nIter/2)
nThin     <- 1
# Timing statements
cat("Estimating", modelFile, "model... \n")
startTime = Sys.time(); print(startTime)
cat("Calling", nChains, "simulations in Stan... \n")
# Fitting model
fit_rl <- stan(modelFile,
data    = data,
chains  = nChains,
iter    = nIter,
warmup  = nWarmup,
thin    = nThin,
init    = "random",
seed    = 1450154626)
# Timing statements
cat("Finishing", modelFile, "model simulation ... \n")
endTime = Sys.time()
print(endTime)
# Corrected time difference display
time_difference <- endTime - startTime
cat("It took", as.numeric(time_difference, units = "secs"), "seconds\n")
# Get summary of model
print(fit_rl)
# Trace
plot_trace_excl_warm_up <- stan_trace(fit_rl,
pars = c('alpha','tau'),
inc_warmup = F)
plot_trace_excl_warm_up
plot_dens <- stan_plot(
fit_rl,
pars = c('alpha',
'tau',
'alpha_prior',
'tau_prior'),
show_density = T,
fill_color = 'skyblue',
alpha = 0.15
) +
geom_vline(
xintercept = tau,
color = "orange",
linetype = "dashed",
linewidth = 1.5
) +
geom_vline(
xintercept = alpha,
color = "forestgreen",
linetype = "dashed",
linewidth = 1.5
)+
labs(title = "Low-High", # Remember to change setting depending on what youre running
subtitle = "Tau (orange): 0.5, Alpha (green): 0.8") +
theme(plot.title = element_text(face = "bold"))
plot_dens
rm(list = ls())
set.seed(420)
library(pacman)
pacman::p_load(rstan,
ggplot2)
# -------- PARAMS TO RUN SETTINGS, UNHASH TO GET SPECIFICS
# Parameters
#tau <- 2  # Temperature parameter for softmax function
#alpha <- 0.3  # Learning rate
# # High-High
# tau <- 9
# alpha <- 0.8
# # Low-Low
# tau <- 0.5
# alpha <- 0.1
# # High-Low
# tau <- 9
# alpha <- 0.1
# Low-High
# tau <- 0.5
# alpha <- 0.8
# ------------------------------------
# Defining function for getting agent + opponent responses
simulate_agents_resp <- function(trials,              # Number of trials
alpha,               # Learning Rate
tau,                 # Inverse temperature
opponent_bias){      # Put opponent's bias here (e.g. 0.7, 0.5...)
# First, generate opponent's choices
opponent_choices <- rbinom(n = trials,
size = 1,
prob = opponent_bias)
# Initialize values
choice <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
choice[1] <- rbinom(1,
size = 1,
prob = 0.5)  # First choice randomly sampled
# ----- The Recorla-Wagner model
# Initialize values for the Rescorla-Wagner model
v <- c(0.5,
0.5)  # Initial values for the agent's choices (heads and tails)
rewards <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
# Simulate agent's responses
for (t in 2:length(opponent_choices)) {  # Starting from the second round
# Calculate reward
if (opponent_choices[t - 1] == choice[t - 1]) {
reward <- 1
} else {
reward <- 0
}
rewards[t - 1] <- reward
# Rescorla-Wagner model
pe <- rewards[t - 1] - v[choice[t - 1] + 1]
v[choice[t - 1] + 1] <- v[choice[t - 1] + 1] + alpha * pe
# Update choice based on softmax
prob_head <- 1 - exp(-tau * v[1]) / (exp(-tau * v[1]) + exp(-tau * v[2]))
choice[t] <- rbinom(1, size = 1, prob = prob_head)
}
# Display the rewards, choices, and updated values
print(rewards)
print(choice)
print(v)
choice = choice + 1
## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h.
data <- list(
nTrials = trials,  # n of trials
choice = choice,
reward = rewards)
return(list(data = data, tau = tau, alpha = alpha))
}
result = simulate_agents_resp(trials = 120,
alpha = 0.6,
tau = 2,
opponent_bias = 0.8)
data = result$data
alpha = result$alpha
tau = result$tau
# -------- PARAMS TO RUN SETTINGS, UNHASH TO GET SPECIFICS
# Parameters
#tau <- 2  # Temperature parameter for softmax function
#alpha <- 0.3  # Learning rate
# # High-High
# tau <- 9
# alpha <- 0.8
# # Low-Low
# tau <- 0.5
# alpha <- 0.1
# # High-Low
# tau <- 9
# alpha <- 0.1
# Low-High
# tau <- 0.5
# alpha <- 0.8
# ------------------------------------
# Defining function for getting agent + opponent responses
simulate_agents_resp <- function(trials,              # Number of trials
alpha,               # Learning Rate
tau,                 # Inverse temperature
opponent_bias){      # Put opponent's bias here (e.g. 0.7, 0.5...)
# First, generate opponent's choices
opponent_choices <- rbinom(n = trials,
size = 1,
prob = opponent_bias)
# Initialize values
choice <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
choice[1] <- rbinom(1,
size = 1,
prob = 0.5)  # First choice randomly sampled
# ----- The Recorla-Wagner model
# Initialize values for the Rescorla-Wagner model
v <- c(0.5,
0.5)  # Initial values for the agent's choices (heads and tails)
rewards <- numeric(length(opponent_choices)) # initializing a vector of zeros the length of how many choices we have
# Simulate agent's responses
for (t in 2:length(opponent_choices)) {  # Starting from the second round
# Calculate reward
if (opponent_choices[t - 1] == choice[t - 1]) {
reward <- 1
} else {
reward <- 0
}
rewards[t - 1] <- reward
# Rescorla-Wagner model
pe <- rewards[t - 1] - v[choice[t - 1] + 1]
v[choice[t - 1] + 1] <- v[choice[t - 1] + 1] + alpha * pe
# Update choice based on softmax
prob_head <- 1 - exp(-tau * v[1]) / (exp(-tau * v[1]) + exp(-tau * v[2]))
choice[t] <- rbinom(1, size = 1, prob = prob_head)
}
# Display the rewards, choices, and updated values
print(rewards)
print(choice)
print(v)
choice = choice + 1
## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h.
data <- list(
nTrials = trials,  # n of trials
choice = choice,
reward = rewards)
return(list(data = data, tau = tau, alpha = alpha))
}
result = simulate_agents_resp(trials = 120,
alpha = 0.6,
tau = 2,
opponent_bias = 0.8)
data = result$data
alpha = result$alpha
tau = result$tau
modelFile <- 'RW.stan'
# Setting specs for modelling
nIter     <- 2000
nChains   <- 4
nWarmup   <- floor(nIter/2)
nThin     <- 1
# Timing statements
cat("Estimating", modelFile, "model... \n")
startTime = Sys.time(); print(startTime)
cat("Calling", nChains, "simulations in Stan... \n")
# Fitting model
fit_rl <- stan(modelFile,
data    = data,
chains  = nChains,
iter    = nIter,
warmup  = nWarmup,
thin    = nThin,
init    = "random",
seed    = 1450154626)
# Timing statements
cat("Finishing", modelFile, "model simulation ... \n")
endTime = Sys.time()
print(endTime)
# Corrected time difference display
time_difference <- endTime - startTime
cat("It took", as.numeric(time_difference, units = "secs"), "seconds\n")
# Get summary of model
print(fit_rl)
# Trace
plot_trace_excl_warm_up <- stan_trace(fit_rl,
pars = c('alpha','tau'),
inc_warmup = F)
plot_trace_excl_warm_up
plot_dens <- stan_plot(
fit_rl,
pars = c('alpha',
'tau',
'alpha_prior',
'tau_prior'),
show_density = T,
fill_color = 'skyblue',
alpha = 0.15
) +
geom_vline(
xintercept = tau,
color = "orange",
linetype = "dashed",
linewidth = 1.5
) +
geom_vline(
xintercept = alpha,
color = "forestgreen",
linetype = "dashed",
linewidth = 1.5
)+
labs(title = "Low-High", # Remember to change setting depending on what youre running
subtitle = "Tau (orange): 0.5, Alpha (green): 0.8") +
theme(plot.title = element_text(face = "bold"))
plot_dens
plot_dens <- stan_plot(
fit_rl,
pars = c('alpha',
'tau',
'alpha_prior',
'tau_prior'),
show_density = T,
fill_color = 'skyblue',
alpha = 0.15
) +
geom_vline(
xintercept = tau,
color = "orange",
linetype = "dashed",
linewidth = 1.5
) +
geom_vline(
xintercept = alpha,
color = "forestgreen",
linetype = "dashed",
linewidth = 1.5
)+
labs(title = "Parameter Recovery", # Remember to change setting depending on what youre running
subtitle = "Tau (orange): 2, Alpha (green): 0.6") +
theme(plot.title = element_text(face = "bold"))
plot_dens
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
knitr::opts_chunk$set(echo = TRUE)
library(lmerTest)
# Load the data
df = read_csv("life_data.csv")
df2 <- df[, c("life_expectancy", "total_expenditure"), drop = FALSE]
xbar = mean(df2$total_expenditure)
